Apologies for the confusion caused. You are correct that the kafkaParams variable is defined in your code
but not explicitly used. It seems that the Kafka parameters are defined but not utilized within the code
snippet you provided.

In the given code, the Kafka parameters are defined but not directly used to configure the Kafka reader.
Instead, the Kafka configuration properties are specified directly within the format("kafka") option of the
readStream method call.

To utilize the kafkaParams in your code, you can modify the readStream method call as follows:

val df = spark.readStream
  .format("kafka")
  .options(kafkaParams) // Pass the kafkaParams as options
  .option("subscribe", topic)
  .option("startingOffsets", "earliest")
  .load()
  .select(from_json(col("value").cast("string"), schema).as("data"))
  .selectExpr("data.*")

By passing the kafkaParams map using the .options() method, you can use the defined Kafka parameters within
your code.

Alternatively, if you prefer not to use the kafkaParams map, you can directly specify the Kafka configuration
properties within the options() method calls, like you have done in the provided code.

val df = spark.readStream
  .format("kafka")
  .option("bootstrap.servers", "ip-172-31-3-80.eu-west-2.compute.internal:9092")
  .option("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
  .option("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
  .option("group.id", "group1")
  .option("auto.offset.reset", "earliest")
  .option("enable.auto.commit", false)
  .option("subscribe", topic)
  .option("startingOffsets", "earliest")
  .load()
  .select(from_json(col("value").cast("string"), schema).as("data"))
  .selectExpr("data.*")

In either case, please ensure that the Kafka configuration properties are accurately set to match your
Kafka cluster configuration.